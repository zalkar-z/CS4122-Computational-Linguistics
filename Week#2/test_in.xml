      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29286">{{redirect|Lexer|people with this name|Lexer (surname)}}
In [[computer science]], '''lexical analysis''', '''lexing''' or '''tokenization''' is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens ([[String (computer science)|strings]] with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a ''lexer'', ''tokenizer'',&lt;ref&gt;{{cite web|url=http://www.cs.man.ac.uk/~pjj/farrell/comp3.html|title=Anatomy of a Compiler and The Tokenizer|website=www.cs.man.ac.uk}}&lt;/ref&gt; or ''scanner'', though ''scanner'' is also a term for the first stage of a lexer. A lexer is generally combined with a [[parser]], which together analyze the [[Syntax (programming languages)|syntax of programming language]]s, web pages, and so forth.
