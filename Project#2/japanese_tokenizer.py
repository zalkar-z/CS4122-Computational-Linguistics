#!/usr/bin/env python3
#
# japanese_tokenizer.py - Computational Linguistics Project#2
# Author: Zalkar Ziiaidin uulu (zalkar@bennington.edu)
# Date Created: 03/11/2019
#

# Notes
#
# In the input file, treat each line as a seperate input
#
# Print different lines to the output file
#
# in the japanese dictionary, each lines is a unique word


# Preliminary Solution:
#
# 1) Read the input file line by line (remember that each line is seperate)
# 2) Using the greedy MaxMatch algorithm try to find the longest existing word
# (check from japanese_wordlist.txt, you can just import it as a variable) and keep seperating words by black line as needed
# 3) Print eveything out
